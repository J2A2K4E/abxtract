# PLS DO NOT TOUCH OR RUN
import simplejson as json
import oauth2
import time
import json
import os
import requests
import pandas as pd
from datetime import datetime, date, timedelta

CONSUMER_KEY = '2972187968-JmUT3zeu2UyVIw315arct0tFjFO0J92r5ozwE1W'
CONSUMER_SECRET = 'FLqXC5f1UGUX4AwaKURWNDSUrB2FkRDajZemus9TKWhwY'
ACCESS_KEY = 'Z0ITlc1Fp5fAjZmX1ks8OzM2j'
ACCESS_SECRET = 'aLqR5uoV2vLLHCkIFmej3R43WGrqrfluZAguHuRRq0Xk8ScDBa'

# PLS DO NOT TOUCH OR RUN
def get_history_sentiment():
    coin_history = db(db.coin_history.id.belongs([10,11,12,13,14,25,26,27,28,29,30,31,41,42])).select(orderby=db.coin_history.price_date)
    for row in coin_history:
        coin_id = row.coin_id
        coin_symbol = db(db.master.id == coin_id).select().first().symbol
        date1 = datetime(row.price_date.year, row.price_date.month, row.price_date.day)
        date2 = date1 + timedelta(days=1)
        tweets = db((db.tweet.coin_symbol==coin_symbol)&(db.tweet.created_at >= date1)&(db.tweet.created_at < date2)).select()
        bullish_count = 0
        bearish_count = 0
        neutral_count = 0
        for tweet in tweets:
            if tweet.sentiment:
                if tweet.sentiment == 1:
                    bullish_count += 1
                elif tweet.sentiment == -1:
                    bearish_count += 1
                else:
                    neutral_count += 1
            else:
                sentiment = get_sentiment_results(tweet.body)
                if sentiment['prediction'] == 'Positive':
                    tweet.update_record(sentiment=1)
                    bullish_count += 1
                elif sentiment['prediction'] == 'Negative':
                    tweet.update_record(sentiment=-1)
                    bearish_count += 1
                else:
                    tweet.update_record(sentiment=0)
                    neutral_count += 1
                db.commit()

        row.update_record(bullish_count=bullish_count, bearish_count=bearish_count, neutral_count=neutral_count)
        db.commit()
    return 'success'


def getHistoryPrice():
    coin_id = 243 # ethereum
    coin_id = 2 # litecoin
    coin_id = 110 # monero
    coingecko_id = db(db.master.id==coin_id).select(db.master.coingecko_id).first().coingecko_id
    startDate = datetime(2019, 3, 29).date()
    endDate = datetime.now().date()
    time.sleep(0.60) # delay
    url = 'https://api.coingecko.com/api/v3/coins/'+coingecko_id+'/market_chart?vs_currency=usd&days=max'
    page = json.loads(requests.get(url).content)
    prices = page['prices']
    for price in prices:
        priceDate = datetime.fromtimestamp(price[0] / 1000).date()
        if priceDate >= startDate and priceDate <= endDate:
            db.coin_history.update_or_insert((db.coin_history.coin_id==coin_id)&(db.coin_history.price_date==priceDate), coin_id=coin_id, price_date=priceDate, price_usd=price[1])
    return locals()

def get_newer_tweets():
    consumer = oauth2.Consumer(key=ACCESS_KEY, secret=ACCESS_SECRET)
    token = oauth2.Token(key=CONSUMER_KEY, secret=CONSUMER_SECRET)
    client = oauth2.Client(consumer, token)
    searchTerm = 'monero'
    symbol = "XMR"
    newest_tweet_id = db(db.tweet.coin_symbol==symbol).select(orderby=~db.tweet.tweet_id).first().tweet_id
    url = "https://api.twitter.com/1.1/search/tweets.json?q="+searchTerm+"&since_id="+newest_tweet_id+"&result_type=recent&count=100&lang=en"
    page_count = 1
    max_page_count = 2000
    attempt_count = 1
    max_attempt_count = 3
    while((page_count <= max_page_count)&(attempt_count <= max_attempt_count)):
        time.sleep(5) # Set time lag
        resp, content = client.request(url, method="GET", headers=None)
        try:
            tweets = json.loads(content, encoding='utf-8')
            tweet_statuses = tweets['statuses']
            for line in tweet_statuses:
                db.tweet.update_or_insert((db.tweet.tweet_id == line['id']),
                                      tweet_id = line['id'],
                                      coin_symbol = symbol,
                                      body = line['text'],
                                      created_at = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(line['created_at'],'%a %b %d %H:%M:%S +0000 %Y')),
                                      retweet_count = line['retweet_count'],
                                      favorite_count = line['favorite_count'],
                                      user_id = line['user']['id'],
                                      user_followers_count = line['user']['followers_count'])
                db.commit()

            if tweets['search_metadata']['since_id'] == 0:
                break
            else:
                next_results = tweets['search_metadata']['next_results']
                url = "https://api.twitter.com/1.1/search/tweets.json"+next_results

            page_count += 1
            attempt_count = 1
        except:
            attempt_count += 1
    return locals()

def get_old_tweets():
    consumer = oauth2.Consumer(key=ACCESS_KEY, secret=ACCESS_SECRET)
    token = oauth2.Token(key=CONSUMER_KEY, secret=CONSUMER_SECRET)
    client = oauth2.Client(consumer, token)
    tweet_list = []
    searchTerm = 'monero'
    url = "https://api.twitter.com/1.1/search/tweets.json?q="+searchTerm+"&result_type=recent&count=100&lang=en"
    page_count = 1
    max_page_count = 4000
    attempt_count = 1
    max_attempt_count = 5

    while((page_count <= max_page_count)&(attempt_count <= max_attempt_count)):

        time.sleep(5) # Set time lag
        resp, content = client.request(url, method="GET", headers=None)
        try:
            tweets = json.loads(content, encoding='utf-8')
            next_results = tweets['search_metadata']['next_results']
            url = "https://api.twitter.com/1.1/search/tweets.json"+next_results

            # Convert to list
            tweet_list = []
            tweet_list.extend(tweets['statuses'])

            #Append list to txt file
            data_dir = os.path.join(request.folder,'static')
            with open(os.path.join(data_dir, 'twitter_'+searchTerm+'.txt'), 'a') as file:
                for tweet in tweet_list:
                    file.write(json.dumps(tweet)+"\n")

            print(page_count)
            print(tweets['statuses'][-1]['created_at'])
            page_count += 1
            attempt_count = 1
        except:
            attempt_count += 1
    return 'success'

def read_stocktwits_txt():
    data_dir = os.path.join(request.folder,'static')
    filename = 'stocktwits_LTC.X.txt'
    testlist = []
    with open(os.path.join(data_dir, filename), 'r') as f:
        for line in f:
            testlist.append(line)
    listCount = len(testlist)
    lastRow = json.loads(testlist[-1])
    return dict(listCount=listCount, lastRow=lastRow, lastRowDate=lastRow['created_at'])
